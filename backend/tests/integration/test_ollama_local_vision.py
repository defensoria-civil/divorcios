"""
Tests de integraciÃ³n para Ollama Local (embeddings) y VisiÃ³n Cloud.

Estos tests verifican:
- Embeddings con nomic-embed-text:latest en Ollama local
- AnÃ¡lisis de imÃ¡genes con qwen3-vl:235b-cloud en Ollama Cloud
- OCR real de documentos usando visiÃ³n

Ejecutar con: pytest tests/integration/test_ollama_local_vision.py -v -s
"""
import pytest
import asyncio
from infrastructure.ai.ollama_client import OllamaClient
from infrastructure.ai.ollama_vision_client import OllamaVisionClient
from core.config import settings


@pytest.mark.asyncio
@pytest.mark.requires_ollama
async def test_ollama_local_embeddings():
    """Test de embeddings con nomic-embed-text en Ollama local"""
    client = OllamaClient(model="nomic-embed-text:latest")
    
    texts = [
        "Divorcio de mutuo acuerdo",
        "Acta de matrimonio",
        "DNI argentino"
    ]
    
    print(f"\nðŸ”„ Testing embeddings with nomic-embed-text:latest locally...")
    try:
        embeddings = await client.embed(texts)
        
        print(f"âœ“ Generated {len(embeddings)} embeddings")
        print(f"âœ“ First embedding dimension: {len(embeddings[0])}")
        
        assert len(embeddings) == 3
        assert all(len(emb) > 0 for emb in embeddings)
        assert isinstance(embeddings[0][0], float)
        
        print("âœ… Local embeddings working correctly!")
    except Exception as e:
        pytest.skip(f"Local embeddings not available: {e}")


@pytest.mark.asyncio
async def test_vision_cloud_simple_image():
    """Test de visiÃ³n cloud con imagen simple"""
    if not settings.ollama_cloud_api_key:
        pytest.skip("OLLAMA_CLOUD_API_KEY not configured")
    
    vision = OllamaVisionClient()
    
    # Crear una imagen de prueba con texto
    from io import BytesIO
    from PIL import Image, ImageDraw, ImageFont
    
    # Imagen 200x100 con texto "DIVORCIO"
    img = Image.new('RGB', (200, 100), color='white')
    draw = ImageDraw.Draw(img)
    
    # Texto grande y legible
    try:
        # Intentar usar una fuente mÃ¡s grande
        draw.text((20, 30), "DIVORCIO", fill='black')
    except:
        # Fallback si no hay fuente disponible
        draw.text((20, 30), "DIVORCIO", fill='black')
    
    buffer = BytesIO()
    img.save(buffer, format='PNG')
    image_bytes = buffer.getvalue()
    
    prompt = "Â¿QuÃ© palabra estÃ¡ escrita en esta imagen? Responde solo con la palabra."
    
    print(f"\nðŸ”„ Testing qwen3-vl:235b-cloud OCR...")
    try:
        response = await vision.analyze_image(
            image_bytes, 
            prompt, 
            model="qwen3-vl:235b-cloud"
        )
        
        print(f"âœ“ Response: {response}")
        assert response is not None
        assert len(response) > 0
        
        # Verificar que detectÃ³ algo relacionado con "DIVORCIO"
        if "divorcio" in response.lower():
            print("âœ… OCR detected text correctly!")
        else:
            print(f"âš ï¸ OCR response: {response} (expected 'DIVORCIO')")
        
    except Exception as e:
        print(f"âš ï¸ Vision cloud error: {e}")
        pytest.skip(f"Vision cloud not available: {e}")


@pytest.mark.asyncio
async def test_vision_cloud_real_ocr():
    """Test de OCR real con formato estructurado JSON"""
    if not settings.ollama_cloud_api_key:
        pytest.skip("OLLAMA_CLOUD_API_KEY not configured")
    
    vision = OllamaVisionClient()
    
    # Crear una imagen simulando un documento con datos
    from io import BytesIO
    from PIL import Image, ImageDraw
    
    img = Image.new('RGB', (400, 300), color='white')
    draw = ImageDraw.Draw(img)
    
    # Simular datos de un documento
    text_lines = [
        "DNI: 12345678",
        "NOMBRE: JUAN PEREZ",
        "APELLIDO: GOMEZ",
        "FECHA NAC: 15/03/1980"
    ]
    
    y_pos = 50
    for line in text_lines:
        draw.text((50, y_pos), line, fill='black')
        y_pos += 40
    
    buffer = BytesIO()
    img.save(buffer, format='PNG')
    image_bytes = buffer.getvalue()
    
    prompt = """Extrae los datos del documento y devuÃ©lvelos en formato JSON con esta estructura:
{
  "dni": "nÃºmero del DNI",
  "nombre": "nombre completo",
  "apellido": "apellido",
  "fecha_nacimiento": "fecha en formato DD/MM/YYYY"
}

Responde SOLO con el JSON, sin texto adicional."""
    
    print(f"\nðŸ”„ Testing structured OCR extraction...")
    try:
        response = await vision.analyze_image(
            image_bytes, 
            prompt, 
            model="qwen3-vl:235b-cloud"
        )
        
        print(f"âœ“ Raw response:\n{response}")
        
        # Intentar parsear como JSON
        import json
        import re
        
        # Buscar JSON en la respuesta
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            print(f"âœ“ Parsed JSON: {data}")
            
            # Verificar campos esperados
            assert "dni" in data or "DNI" in data
            print("âœ… Structured OCR working!")
        else:
            print(f"âš ï¸ No JSON found in response, but got: {response[:200]}")
        
    except Exception as e:
        print(f"âš ï¸ Structured OCR error: {e}")
        pytest.skip(f"Structured OCR not working: {e}")


@pytest.mark.asyncio
async def test_vision_multimodal_reasoning():
    """Test de razonamiento multimodal con imagen"""
    if not settings.ollama_cloud_api_key:
        pytest.skip("OLLAMA_CLOUD_API_KEY not configured")
    
    vision = OllamaVisionClient()
    
    # Crear una imagen con formas geomÃ©tricas
    from io import BytesIO
    from PIL import Image, ImageDraw
    
    img = Image.new('RGB', (300, 300), color='white')
    draw = ImageDraw.Draw(img)
    
    # Dibujar un cÃ­rculo rojo y un cuadrado azul
    draw.ellipse([50, 50, 150, 150], fill='red', outline='black')
    draw.rectangle([170, 170, 270, 270], fill='blue', outline='black')
    
    buffer = BytesIO()
    img.save(buffer, format='PNG')
    image_bytes = buffer.getvalue()
    
    prompt = "Describe las formas geomÃ©tricas en la imagen. Â¿CuÃ¡ntas hay y de quÃ© colores son?"
    
    print(f"\nðŸ”„ Testing multimodal reasoning...")
    try:
        response = await vision.analyze_image(
            image_bytes, 
            prompt, 
            model="qwen3-vl:235b-cloud"
        )
        
        print(f"âœ“ Response: {response}")
        
        # Verificar que mencionÃ³ formas y colores
        response_lower = response.lower()
        mentions_shapes = any(word in response_lower for word in ['cÃ­rculo', 'circle', 'cuadrado', 'square', 'rectÃ¡ngulo'])
        mentions_colors = any(word in response_lower for word in ['rojo', 'red', 'azul', 'blue'])
        
        if mentions_shapes and mentions_colors:
            print("âœ… Multimodal reasoning working!")
        else:
            print(f"âš ï¸ Response may be incomplete: {response}")
        
        assert response is not None
        
    except Exception as e:
        print(f"âš ï¸ Multimodal reasoning error: {e}")
        pytest.skip(f"Multimodal reasoning not available: {e}")


@pytest.mark.asyncio
@pytest.mark.requires_ollama
async def test_embedding_similarity():
    """Test de similitud semÃ¡ntica con embeddings"""
    client = OllamaClient(model="nomic-embed-text:latest")
    
    # Textos similares y diferentes
    texts = [
        "Solicitud de divorcio de mutuo acuerdo",
        "PeticiÃ³n de divorcio consensuado",
        "Compra de automÃ³vil usado"
    ]
    
    print(f"\nðŸ”„ Testing semantic similarity...")
    try:
        embeddings = await client.embed(texts)
        
        # Calcular similitud coseno entre embeddings
        import numpy as np
        
        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
        
        sim_1_2 = cosine_similarity(embeddings[0], embeddings[1])
        sim_1_3 = cosine_similarity(embeddings[0], embeddings[2])
        
        print(f"âœ“ Similarity (divorcio vs divorcio): {sim_1_2:.4f}")
        print(f"âœ“ Similarity (divorcio vs automÃ³vil): {sim_1_3:.4f}")
        
        # Los dos textos sobre divorcio deben ser mÃ¡s similares
        assert sim_1_2 > sim_1_3, "Semantic similarity not working correctly"
        print("âœ… Semantic similarity working!")
        
    except Exception as e:
        pytest.skip(f"Semantic similarity not available: {e}")


if __name__ == "__main__":
    print("ðŸš€ Running Ollama Local + Vision Cloud Integration Tests")
    print("=" * 60)
    pytest.main([__file__, "-v", "-s"])
